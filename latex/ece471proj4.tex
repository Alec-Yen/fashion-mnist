% --------------------------------------------------------------------
% Preamble
% --------------------------------------------------------------------
\documentclass[paper=a4, fontsize=11pt,twoside]{scrartcl}	% KOMA

\usepackage[a4paper,pdftex]{geometry}	% A4paper margins
\setlength{\oddsidemargin}{5mm}			% Remove 'twosided' indentation
\setlength{\evensidemargin}{5mm}

\usepackage[english]{babel}
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm,amssymb,bm}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\newcommand{\figref}[1]{\figurename~\ref{#1}}
\newcommand{\tabref}[1]{\tablename~\ref{#1}}

% --------------------------------------------------------------------
% Definitions (do not change this)
% --------------------------------------------------------------------
\newcommand{\HRule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\makeatletter							% Title
\def\printtitle{%						
    {\centering \@title\par}}
\makeatother									

\makeatletter							% Author
\def\printauthor{%					
    {\centering \large \@author}}				
\makeatother							

% --------------------------------------------------------------------
% Metadata (Change this)
% --------------------------------------------------------------------
\title{	\normalsize \textsc{ECE 471: Intro to Pattern Recognition \\ Project 4} 	% Subtitle
		 	\\[2.0cm]								% 2cm spacing
%			\HRule{0.5pt} \\						% Upper rule
			\LARGE \textbf{{Neural Network, Decision Tree, and Performance Evaluation}}	% Title
%			\HRule{2pt} \\ [0.5cm]		% Lower rule + 0.5cm spacing
		}

\author{
		Alec Yen\\	
		University of Tennessee, Knoxville\\	
		Electrical Engineering and Computer Science Department\\
        ayen1@vols.utk.edu \\
}


\begin{document}
% ------------------------------------------------------------------------------
% Maketitle
% ------------------------------------------------------------------------------
\thispagestyle{empty}		% Remove page numbering on this page

\printtitle					% Print the title data as defined above
\vfill
\printauthor				% Print the author data as defined above
\newpage
% ------------------------------------------------------------------------------
% Begin document
% ------------------------------------------------------------------------------
\setcounter{page}{1}		% Set page numbering to begin on this page

%\tableofcontents

\section{Abstract}
In this report, procedures for KNN, decision trees, and neural networks were implemented and tested on a forensic glass dataset. All algorithms were evaluated for performance using m-fold cross validation and an 80-10-10 split was also tested on a 3-layer neural network. Each learning technique did similarly well, although KNN performed slightly better than decision trees and neural networks for this dataset. Smaller values of k appeared to achieve higher accuracy than larger values. M-fold cross validation also showed that for the neural network, using more nodes improved the accuracy of the network.


\section{Introduction}


Some of the most popular pattern classification techniques include decision trees and neural networks. Both fall under the category of supervised learning. However, their methodologies differ from past techniques. Decision trees aims to differentiate elements of a dataset using property queries from parent to child nodes. Neural networks are biologically-inspired models based on the fundamental building blocks of the human brain (perceptrons, axons, dentrites, etc.).

Decision trees have been used in a variety of applications, including modelling rainfall predictions \cite{geetha2014data} and even evaluating a nation's judicial system \cite{tsai2010performance}. While decision trees have practical applications, neural networks are what has truly caught the eye of the pattern classification field in recent decades. Their usage has been extensive and ubiquitous including applications from reading facial expressions \cite{kobayashi1993dynamic} to fault diagnosis \cite{gupta2015fault}.

The objective of this project was to implement cross validation for the performance evaluation of KNN, decision trees, and a 3-layer neural network on a forensic glass dataset. KNN was implemented from scratch and the decision tree and neural network were implemented using \texttt{scikit-learn} and \texttt{keras}, respectively.


\section{Technical Approach}

\subsection{KNN}
K-nearest neighbors (KNN) was implemented in a previous project. This algorithm was reused for the purpose of testing cross validation; a summary of the methodology follows.

In KNN, a test sample's Euclidean distance from each training sample is measured. The $k$ nearest training samples' class labels are found and the majority is assigned to the test sample. The posterior probability of a tests sample is justified as in \eqref{eq:knn}, where $k_i$ are the $k$ closest training samples of class $i$. $n_i$ is the total number of training samples of class $i$ and $n$ is the total number of training samples, and thus $\frac{n_i}{n}$ represents the prior probability. Thus, KNN assumes a prior probability based on the distribution of the training data. If we wish to alter the prior probability to a new desired prior probability of $P(\omega_i)'$, we must multiply $\frac{k_i}{k}$ by $\frac{P(\omega_i)'}{P(\omega_i)}$. KNN is non-parametric because we do not need a model to calculate maximum posterior probability and perform classification.

\begin{equation}
\label{eq:knn}
P(\omega_{i}|\mathbf{x}) = \frac{p(\mathbf{x}|\omega_{i})P(\omega_{i})}{p(\mathbf{x})}
= \frac{\frac{k_i/n_i}{V}\frac{n_i}{n}}{\frac{k/n}{V}} = \frac{k_i}{k}
\end{equation}


\subsection{Decision Trees}
Decision trees are a non-statistical approach, unlike KNN. All samples start at the root node and are divided up and classified as one progresses through the tree. At each node $N$, a property query is made to distinguish each sample into two groups. The impurity $i(N)$ of such a decision can be measured using either entropy impurity \eqref{eq:entropy} or Gini impurity \eqref{eq:gini}. Regardless of which type to use, the objective of decision trees is to maximize the change in impurity from each node to the next layer. This change in impurity is defined as in \eqref{eq:impuritychange}. Decision trees in the project were implemented using \texttt{scikit-learn}'s \texttt{DecisionTreeClassifier} class.

\begin{equation}
\label{eq:entropy}
i(N) = -\sum_{j}P(\omega_j)\log_2P(\omega_j)
\end{equation}

\begin{equation}
\label{eq:gini}
i(N) = 1 - \sum_{j}P^2(\omega_j)
\end{equation}

\begin{equation}
\label{eq:impuritychange}
\Delta i(N) = i(N) - P_Li(N_L) - (1-P_L)i(N_R)
\end{equation}


\subsection{Neural Networks}
The fundamental building block of neural networks is the perceptron (a single layer network), which are inspired by the neurons of the human brain. They are composed of inputs $\vec{x} = \begin{bmatrix} x_1 & x_2 & \dots & x_d & 1 \end{bmatrix}$ and weights $\vec{w} = \begin{bmatrix} w_1 & w_2 & \dots & w_d & -w_0 \end{bmatrix}$, where $w_0$ is the bias. The output $z=\vec{w}^T\vec{x}$. If $z>0$, the perceptron outputs 1. Otherwise, it outputs 0. Assuming the ground truth is $\vec{T}$ and the network's output is $\vec{z}$, gradient descent can be used as in \eqref{eq:perceptron} to converge to a solution.

\begin{equation}
\label{eq:perceptron}
\vec{w}^{k+1}=\vec{w}^k + \sum_{i=1}^n(T_i-z_i)x_i
\end{equation}

A 3-layer neural network is simply the combination of these perceptron units with the addition of backpropagation. In this project, the \texttt{keras} library was used to implement the neural network.

\subsection{Cross Validation}
One of the primary goals of this project is to implement a general procedure for m-fold cross validation of a classifier. This serves not to help build a classifier, but rather to evaluate a classifier's performance and maximize the data available. In this process, the dataset is partitioned into $m$ sets. $m-1$ sets are used for training the classifier while the remaining $1$ set is used for evaluating the classifier. The process is then repeated $m$ times so that each set is used for testing at least once. The overall accuracy of the classifier is the average of the accuracies found on each of the $m$ test sets.

\section{Experimentation and Results}
All algorithms were seeded with fixed seeds for reproducibility. The dataset was also normalized before each test.

\subsection{The Dataset}
The \texttt{fglass} data set from Ripley's book was used \cite{Ripley1996PatternRipley}. It is a dataset that classifies different types of glass (window glass, containers, tableware, etc) using various numerical features of the glass, including the refractive index and the weight of different oxides (sodium, magnesium, etc). In total, there are 9 features and 6 possible classes (numbered 1 through 7 and omitting 4). There were 214 total samples and \cite{Ripley1996PatternRipley} also provided the indexes for ten groups for usage in cross-validation experiments.

\subsection{Cross Validation and KNN}
The first task was to implement cross validation using KNN, where $m=10$. The cross-validated accuracy for each k is plotted in \figref{fig:knn_acc_task1}. The exact values are shown in \tabref{tab:knn_acc_task1}. Through m-fold cross validation, it was found that k = \input{figures/best_knn_k.txt}performed the best overall. The standard deviation of the accuracies while performing cross validation is shown in \figref{fig:knn_std_task1} and \tabref{tab:knn_std_task1}.

\begin{figure}[!t!h]
	\centering
	\includegraphics[width=5in]{figures/knn_acc_task1.png}
	\caption{Cross validation accuracy of KNN at different values of k}
	\label{fig:knn_acc_task1}
\end{figure}

\begin{table}[!t!h]
	\centering
\input{figures/knn_acc.txt}
\caption{Cross validation accuracy of KNN at different values of k}
\label{tab:knn_acc_task1}
\end{table}

\begin{figure}[!t!h]
	\centering
	\includegraphics[width=5in]{figures/knn_std_task1.png}
	\caption{Cross validation standard deviation of accuracy of KNN at different values of k}
	\label{fig:knn_std_task1}
\end{figure}

\begin{table}[!t!h]
	\centering
	\input{figures/knn_std.txt}
	\caption{Cross validation standard deviation of accuracy of KNN at different values of k}
	\label{tab:knn_std_task1}
\end{table}

\begin{table}[!t!h]
	\centering
	\input{figures/knn_cm_save.txt}
	\caption{Matrix of accuracy of KNN at different values of k using each group m for cross validation. \tabref{tab:knn_acc_task1} was obtained by taking the average of each row.}
	\label{tab:knn_cm}
\end{table}

\subsection{Cross Validation and Decision Tree}
The second task was to use the existing cross validation procedure to evaluate the accuracy of decision tree is performing classification on this dataset. Using the \texttt{DecisionTreeClassifier} class, the accuracy of the decision tree was \input{figures/dt_acc.txt}. The standard deviation was \input{figures/dt_std.txt}. The code to implement the decision tree is seen in \ref{app:dt}.

\subsection{80-10-10 Split with Neural Network}
The third and final task was to train a 3-layer neural network. There were 9 inputs, $n$ nodes in the hidden layer, and 8 outputs. Although there are 6 classes, one-hot encoding is used where the outputs are zero-indexed and the one with the largest output weight is the network's decision. Because the numbers are from 1 to 7, 8 outputs are used for one-hot encoding.

We were tasked with implementing the network using 80\% of data for training, 10\% for validation, and 10\% for testing. More specifically, of the 10 grouped sets we were provided, 9 groups were used for training/validation and 1 was used for testing. Because the seed was fixed, group $m=8$ was used for testing each time. This turned out to be a good middle ground test set in terms of accuracy (referring to \tabref{tab:knn_cm}).

Using the code in \ref{app:nn}, a 3-layer neural network was trained with these specifications using \texttt{keras}, with varying hidden layer nodes. Using \texttt{keras}, the training set was divided into 8/9 training and 1/9 validation (for 80\% and 10\%). The performance curve in \figref{fig:nn_acc_task3} demonstrates the accuracy using varying number of neurons in the hidden layer. The performance curve in \figref{fig:nn_acc_performance_curve} demonstrates the accuracy of the training set over time.


\begin{figure}[!t!h]
	\centering
	\includegraphics[width=5in]{figures/nn_acc_task3.png}
	\caption{Accuracy using 80-10-10 split with neural network, using a variable number of hidden nodes}
	\label{fig:nn_acc_task3}
\end{figure}

\begin{table}[!t!h]
	\centering
	\input{figures/nn_acc_task3.txt}
	\caption{Accuracy using 80-10-10 split with neural network, using a variable number of hidden nodes}
	\label{tab:nn_acc_task3}
\end{table}

\begin{figure}[!t!h]
	\centering
	\includegraphics[width=5in]{figures/nn_acc_performance_curve.png}
	\caption{Performance curve of accuracy on training set using 80-10-10 split with neural network, using \protect\input{figures/best_nn_n.txt}hidden nodes, over time}
	\label{fig:nn_acc_performance_curve}
\end{figure}

%\subsection{Cross Validation and Neural Network}
%As an additional test, m-fold cross validation was also performed on the neural network. The cross validated accuracies are plotted in \figref{fig:nn_acc_cv}, using using a variable number of hidden nodes. 
%Tests were also done to evaluate performance using more combinations of nodes in the hidden layers. These results are shown in \tabref{tab:nn_acc_cv}.

%\begin{figure}[!t!h]
%	\centering
%	\includegraphics[width=5in]{figures/nn_acc_cv.png}
%	\caption{Cross validation accuracy of neural network, using a variable number of hidden nodes}
%	\label{fig:nn_acc_cv}
%\end{figure}
%
%\begin{table}
%	\centering
%	\input{figures/nn_acc_cv.txt}
%	\caption{Cross validation accuracy of neural network, using a variable number of hidden nodes}
%	\label{tab:nn_acc_cv}
%\end{table}

%\begin{table}
%	\centering
%	\input{figures/nn_acc_cv_tab.txt}
%	\caption{Cross validation accuracy of neural network, using different nodes at 1st layer (rows) and 2nd layer (columns)}
%	\label{tab:nn_acc_cv_tab}
%\end{table}



\section{Discussion}
From the results, we can draw several conclusions regarding the accuracy of each learning technique on the dataset. KNN achieved an accuracy of \input{figures/best_knn_acc.txt}when k = \input{figures/best_knn_k.txt}. The decision tree achieved an accuracy of \input{figures/dt_acc.txt}using cross validation. The neural network achieved an accuracy as high as \input{figures/best_nn_acc.txt}using a \input{figures/best_nn_n.txt}nodes in the hidden layer.

From \tabref{tab:knn_cm}, it is clear that some of the groups were more difficult to test on, resulting in lower accuracies. In particular, group $m=5$ had very low accuracies regardless of the $k$ used. On the other hand, $m=10$ had very high accuracy by comparison.

Smaller values of $k$ appeared to do better. This may indicate that in the hyperspace of the test samples, there is a large degree of mixing between the samples, so there are no clearly defined clusters. However, the strong performance of KNN indicates that the nearest neighbors did indeed serve as a good classification method.

The decision tree performed comparably to KNN. \texttt{scikit-learn}'s method achieved an accuracy of \input{figures/dt_acc.txt}which is slightly under than of KNN's best performance. Nonetheless, it is very comparable which suggests that the tree was able to obtain relatively high purity property queries.

A greater number of nodes in the neural network clearly helped the accuracy, as seen in \figref{fig:nn_acc_task3}. The greater complexity allowed the network to become more nuanced and develop more decision boundaries that helped address the nonlinear nature of the data. Of course, using a larger number of nodes risks over-fitting, but that effect can be mitigated with the usage of a validation set.


\section{Summary}
In this project, three different classifiers have been implemented and evaluated using m-fold cross validation. It is observed that KNN has the strongest performance for smaller values of $k$ and that decision trees can obtain a comparable accuracy. Neural networks struggled significantly if not given enough hidden nodes, but with greater complexity, the 3-layer neural network also achieved fairly high accuracy.

The classifiers and algorithms in this project have been very thoroughly tested. The modular design and implementation of the Python algorithms benefited this process nicely. Future work on this project could be done to test other parameters of the decision tree and neural network classifiers, including using different activation functions or perhaps introducing more layers. Overall, this project served as a meaningful and instructive way to introduce students to decision trees, neural networks, and m-fold cross validation.

% ------------------------------------------------------------------------------
% End document
% ------------------------------------------------------------------------------

\bibliographystyle{IEEEtran}
\bibliography{citation/bib}

% \newpage
\appendix
\section{Appendix}

\subsection{Decision Tree}
\label{app:dt}
\begin{lstlisting}[caption={Implementation of decision tree},captionpos=b,language=Python,breaklines]
clf = DecisionTreeClassifier().fit(tr[:,:-1],tr[:,-1])
Z = clf.predict(te[:,:-1])
\end{lstlisting}


\subsection{Neural Network}
\label{app:nn}
\begin{lstlisting}[caption={Implementation of 3-layer neural network using \texttt{keras} with 9 inputs, variable nodes in the hidden layer, and 8 outputs using one-hot encoding},captionpos=b,language=Python,breaklines]
model = Sequential()
model.add(Dense(units=hidden_layer_inputs, activation='relu', input_dim=9))
model.add(Dense(units=8, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=100, batch_size=32,verbose=False,validation_split=1/9)
loss_and_metrics = model.evaluate(x_test,y_test,batch_size=128,verbose=False)
\end{lstlisting}


\end{document}

